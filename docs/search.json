[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LearningDiary",
    "section": "",
    "text": "About\n\nHi! I’m Mike. I’m an urban planner by training, but my first degree was in English Literature. I have six years of experience working in the creative industry, during which time I did a range of things – copywriting, content strategy, a bit of photography and directing of short films.\nI transitioned to planning in 2019, completing a Master’s degree in City and Regional Planning at the University of Cape Town. Subsequent to that I worked for the Western Cape Provincial government. The highlight of my time there was working on an Inclusionary Housing Policy Framework for the province.\nI’ve always enjoyed data and working with numbers, which is what drew me to my current pursuit: an MSc Urban Spatial Science at University College London’s Centre for Advanced Spatial Analysis. This site is a learning diary for one of my modules, Remotely Sensing cities and Environments. I hope that you enjoy it! It contains my thoughts and learnings for each week of the course. Where possible, I’ve used imagery from Cape Town or the Western Cape to illustrate the techniques we were taught, because I know the area and it helps me to interpret the results.\nFor something a bit different, here’s a video that I worked on with one of my colleagues from the creative industry to describe the Inclusionary Housing Policy Framework that I worked on while at the provincial government:"
  },
  {
    "objectID": "week 1.html",
    "href": "week 1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "2 Summary\nIn my view, the most exciting applications for remote sensing data are in situations where traditional forms of data collection – such as a national census – are unreliable or out of date. This is true of South Africa, where obtaining up-to-date demographic or economic information is very difficult. This has led to one private sector company effectively cornering the market on spatially disaggregated demographic data. I’ll discuss that a bit more in week 4, when I get into policy.\nThere are many applications of Remote Sensing, and they tend to overlap quite a bit, but since the practical covered imagery from Sentinel 2 and Landsat 8, I’ll cover some interesting applications of these two sources of imagery here:"
  },
  {
    "objectID": "week 1.html#what-is-remote-sensing",
    "href": "week 1.html#what-is-remote-sensing",
    "title": "1  Week 1",
    "section": "2.1 What is remote sensing?",
    "text": "2.1 What is remote sensing?\nRemote sensing is a subset of Geographic Information Science (GIS), and broadly refers to the science (and art) of using remote instruments – sensors mounted on satellites or planes – to record information remotely. This week, I will cover some sources of remote sensing data, as well as some applications of its use."
  },
  {
    "objectID": "week 1.html#active-and-passive-sensors",
    "href": "week 1.html#active-and-passive-sensors",
    "title": "1  Week 1",
    "section": "2.2 Active and passive sensors",
    "text": "2.2 Active and passive sensors\nThere are two key types of remote sensors: Passive and active. Passive sensors read light from the sun reflected off the earth’s surface. The images they read can be interfered with by, for example, clouds or atmospheric haze. Active sensors, on the other hand, send a signal down to the earth that bounces back to the satellite’s sensor. These sensors can, as a result, operate at night and see through clouds. The figure below illustrates the difference:\n\n\n\nPassive sensors rely on reflectance from the sun, whereas active sensors send their own signals and are able to see through clouds and gather data at night (Barba-Sevilla and Menezes 2023).\n\n\n\n2.2.1 Electromagnetic signatures\nAs discussed above, passive sensors read radiation reflected from the earth’s surface. Different types of radiation have different wavelengths, and operate in different parts of the electromagnetic spectrum. And different materials reflect different values on different parts of the electromagnetic spectrum. So different sensors can read different wavelengths, which provide different types of information (and operate under different constraints).\nThe figure below shows the electromagnetic spectrum, and different instruments that operate at different points on the spectrum. The earth’s atmosphere blocks most wavelengths on the spectrum, with the exceptions of radio waves, visible light (the ‘optical window’ below), and some parts of the infrared spectrum. Most passive sensors read wavelengths within the optical window, and active sensors operate using radio waves.\n\n\n\nOnly certain types of radiation reach the earth’s surface from space (Geospatial UK 2021).\n\n\n\n\n2.2.2 Resolutions\nRemote sensing data has four resolutions, namely:\nSpectral: The number of bands that the sensor records (for example, red, green, blue for visible light)\nSpatial: The size of each pixel (this can go down to centimetres for very high resolution data or be up to over a kilometre for some purposes)\nTemporal: The frequency with which a given area will be revisited (there is often a tradeoff between spatial and temporal resolution, as a lower spatial resolution allows for more of the earth’s surface to be captured and hence a higher temporal resolution)\nRadiometric: The range of reflectance values"
  },
  {
    "objectID": "week 1.html#reflections",
    "href": "week 1.html#reflections",
    "title": "1  Week 1",
    "section": "3.1 Reflections",
    "text": "3.1 Reflections\nMy first impression was how long a lot of this takes. Working with the raster data in SNAP and then processing it in R is time consuming and convoluted – and it made me look forward to getting into Google Earth Engine later in the term. It’s also just a totally new approach to looking at imagery compared to what I am used to, which feels both exciting and unfamiliar.\nI’ve got experience in photography from my previous career, so it was also interesting to interact with large raster files in a completely different way, and to play around with histograms from the electromagnetic spectrum (my previous exposure to histograms and raster imagery was in trying to get rid of ‘clipping’ in areas of a photograph that are over-or-under exposed – which means that parts of an image are entirely black or white. Working with the images in SNAP and then displaying them in QGIS reminded me of that). It’s also quite cool to see how an image can be ‘built’ from three greyscale RGB bands into a full colour composite – it helped me to better understand how digital images are really just data.\nI’m also quite used to using satellite imagery as a basemap for spatial analysis in my other previous job as an urban planner. There, the purpose was usually to provide context for the viewer looking at the map (or for myself when working). That was often quite high resolution RGB imagery, and I took it for granted that I was able to zoom into a particular location and see what was happening at a very fine scale. It’s quite interesting to change tack here to working with spectral imagery, which has a lower resolution but a lot more information because of the different bands. I’m looking forward to the rest of the module, and learning a whole lot more.\n\n\n\n\nBarba-Sevilla, Magali, and Elizabeth Ashley Menezes. 2023. “GUIDE: FUNDAMENTALS OF SYNTHETIC APERTURE RADAR (SAR).” https://storymaps.arcgis.com/stories/20d8cd2ce11a4d5d81a8a65711d5ec29.\n\n\nGeospatial UK. 2021. “What Is Remote Sensing?” https://www.geospatialuk.org/post/what-is-remote-sensing."
  },
  {
    "objectID": "week 2.html",
    "href": "week 2.html",
    "title": "2  Week 2",
    "section": "",
    "text": "Here is a short presentation discussing the Sentinel 1 mission (technically this is two satellites and not 1):"
  },
  {
    "objectID": "week 3.html",
    "href": "week 3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "4 Week 3: Enhancements"
  },
  {
    "objectID": "week 3.html#summary",
    "href": "week 3.html#summary",
    "title": "3  Week 3",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThere are many different types of correction and enhancement possible for remote sensing. Corrections typically are typically applied to remove flaws created by the sensor, atmosphere, or terrain. However, for the purpose of this week, I am going to focus on a couple of enhancements of remote sensing imagery – namely Ratio, Texture, and Principal Component Analysis (PCA).\n\n4.1.1 Ratio: Normalised Difference Vegetation Index (NDVI)\nRatio enhancements use the different spectral signatures of different materials in spectral bands to exaggerate specific features. NDVI uses red and near-infrared spectral bands to draw out healthy vegetation, because healthy vegetation reflects highly in the near infrared wavelength and is absorbed in the red red one. Areas with a high NDVI index represent healthy vegetation. It’s calculated as follows:\n\\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\nThe image below shows the NDVI for Cape Town using Landsat imagery from 2022. It’s a bit hard to read because of the ocean:\n\n\n\nNormalised Difference Vegetation Index for Cape Town\n\n\nWe can get around this by utilising only NDVI values over 0.15. Here the healthy vegetation in the farmland to the North of the city is emphasised. Urban areas are white, as is the ocean. Interestingly, areas that are mountainous or national parks (and high in indigenous fynbos vegetation) do not register on the NDVI. This includes the Cape Point peninsula in the South West of the image below. This may be due to the time of year and the unharvested agricultural areas being extremely high in NDVI relative to indigenous vegetation:\n\n\n\nNDVI for Cape Town (only showing values above 1.5)\n\n\n\n\n4.1.2 Texture\nTexture is pretty cool. Essentially, it’s a measure of each pixel’s similarity (or difference) to the pixels around it. For example, the image below shows homogeneity for Cape Town. It’s calculated using a grey-level co-occurrence matrix (GLCM). This involves creating a matrix for all pixels and their neighbours in an image, and calculating how homogeneous (similar) each pixel is to its neighbours. In the image below, areas with high homogeneity (the ocean, stretches of agricultural land in the north) are white, while areas with significant contrast (such as the coastline) are dark, due to the high contrast between the ocean and beaches.\n\n\n\nTexture (homogeneity) for Cape Town\n\n\n\n\n4.1.3 PCA\nPrincipal Component Analysis (PCA) is a dimensionality reduction algorithm – it allows you to reduce a high dimensional dataset (like a multi-spectral raster file with many bands) into a lower number of dimensions. The first component (or dimension) is selected to explain as much of the variance as possible in each observation. That’s what this image shows: the first principal component for the imagery used above:\n\n\n\nPCA for Cape Town"
  },
  {
    "objectID": "week 3.html#applications",
    "href": "week 3.html#applications",
    "title": "3  Week 3",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nOf the three techniques I have described above, NDVI probably has the most obvious practical application. It’s easy to see how a vegetation index can be useful for monitoring and predicting crop yields, fire hazards, and drought impacts. This article discusses how it can be used for precision farming by classifying vegetation as this image shows:\n\n\n\nAn example of NDVI applied to agriculture. Irrigated areas show up as brighter in the image (Source: GIS Geography)\n\n\nThe images I’ve generated for texture analysis and PCA are less clear in their immediate use, but that doesn’t mean they aren’t extremely powerful. Texture can, for example, be used for change detection or to classify different land uses. This video, particularly from 17:38 to 20:15, discusses some applications of texture in remote sensing data:\n\nBasically, texture is used to retrieve the areas of an image that are most similar to a target area. It’s very easy to see how that might be relevant to a remote sensing application in an urban environment – it could be used to identify open space, informal areas, flood-prone areas, or a host of different aspects of the built environment.\nPCA can be used for a variety of remote sensing applications, such as feature extraction, change detection, and classification. And it’s not particularly new! Munyati (2004) used PCA to detect wetland change in Zambia using imagery from the 1980s and 90s. The study ran PCA on a 12-band image to generate a colour composite that highlighted change detection in the wetland system over time."
  },
  {
    "objectID": "week 3.html#reflections",
    "href": "week 3.html#reflections",
    "title": "3  Week 3",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nI’ve skirted around corrections here and focused on enhancement, probably because enhancement feels exciting and interesting (they let you draw out interesting features of the data) whereas corrections feel painstaking and time-consuming. I know they’re important, but it’s also really exciting to put PCA into use (even if my example above is pretty trivial and doesn’t really offer much more than a grayscale map of Cape Town).\nI first encountered PCA last term in the Foundations of Spatial Data Science module. At the time I found it completely intimidating and had to really work in order to understand it. Now I think I get it conceptually (although performing the linear algebra used to transform the dimensionality reduction is beyond me), and it’s fun to be able to use it and understand how and why it’s useful.\nSimilarly, I had never encountered texture analysis before, but it’s really interesting to learn about its uses in other disciplines (like medical imaging) and to start to get a sense of its value when applied to remote sensing imagery.\nThis week also made me realise how computationally intensive working with remote sensing data is: these files are big (I don’t want to think about how much hard drive space I’ve used in these first three weeks) and PCA took so long to perform that I ended up just going for a run while my computer finished the process. I’m looking forward to getting into Google Earth Engine when a lot of this will be sped up dramatically.\n\n\n\n\nMunyati, Christopher. 2004. “Use of Principal Component Analysis (PCA) of Remote Sensing Images in Wetland Change Detection on the Kafue Flats, Zambia.” Geocarto International 19 (3): 11–22. https://doi.org/10.1080/10106040408542313."
  },
  {
    "objectID": "week 4.html",
    "href": "week 4.html",
    "title": "4  Week 4",
    "section": "",
    "text": "5 Policy: The City of Cape Town Municipal Spatial Development Framework"
  },
  {
    "objectID": "week 4.html#summary",
    "href": "week 4.html#summary",
    "title": "4  Week 4",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThe City of Cape Town Municipal Spatial Development Framework (MSDF) is a policy document that guides spatial planning and land use decisions for the City of Cape Town. It’s a requirement of the Spatial Planning and Land Use Management Act (SPLUMA) that all municipalities have an MSDF (Republic of South Africa 2013). SPLUMA is pretty interesting legislation. It was introduced in 2013 to rationalise South Africa’s fragmented planning law. One of the most exciting (in theory) things about it is the development principles it introduces. According to law in South Africa, all planning decisions taken by government must take into account the principles of:\n\nSpatial Justice;\nSpatial Sustainability;\nEfficiency;\nSpatial Resilience; and\nGood Administration\n\nPretty cool, right? Except it’s very difficult to define all of these things, and so it’s very common for developers to argue that a car-centric gated development located in the middle of nowhere will contribute to spatial justice and sustainability.\nThat’s why it’s extremely important in South Africa to have clear planning policy that’s informed by evidence (including remote sensing data). I think Cape Town does a pretty decent job in this regard – their policy is generally sound, and informed by strong analysis. Where things can break down is at the interface between policy and individual land use decisions, which get made with little reference to policy or spatial planning principles. So I think there’s a lot of work to be done in reconciling policy with the actual decisions that get taken.\nThe Cape Town MSDF is summed up in this image, which has been nicknamed the ‘blue turtle’ by planners in South Africa – the blue areas (which resemble a turtle) are the areas of the city where development is encouraged:\n\n\n\nThe City of Cape Town MSDF (City of Cape Town 2022)\n\n\nOne of the key issues highlighted in the MSDF is the need to stop outward urban expansion (or sprawl), and to keep developing within the ‘blue turtle’. This has been difficult to do for many reasons, including the increase of gated private developments on the urban periphery. One of the other reasons is a phenomenon that’s been on the rise since COVID-19: informal land invasion. This is a complex issue that is driven by deep poverty, inequality, and housing shortages, and those root causes need to be addressed. However, the consequences of informal expansion are serious, as sites meant for facilities like housing, schools or clinics are no longer available. Last year, Driftsands Nature Reserve had to be decommissioned as it had been occupied illegally, and the ecosystem was degraded beyond rehabilitation.\nThe MSDF highlights this issue as follows:\n\nThe urban development edge has made provision for development opportunities at strategically located expansion areas located close to areas of intense urbanisation pressures, mass land invasion, significant levels of persistent informality and increasing levels of unauthorised occupation of land nullifying scheduled government housing projects (City of Cape Town 2022).\n\nAnd this video talks about the MSDF’s approach to housing, which is where I believe remote sensing data has significant application for the city:"
  },
  {
    "objectID": "week 4.html#applications",
    "href": "week 4.html#applications",
    "title": "4  Week 4",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nThe city uses remote sensing data quite a lot already, and Cape Town has high-resolution LIDAR data that has been used to map informal settlements in the city by Shoko and Smit (2022). I think that, given the issues with housing supply highlighted above, and the increasing issue with informal land invasions happening in the city, the city could explore using remote sensing data to identify areas that may be vulnerable to future invasion.\nThis is an idea that was proposed by Tellman, Eakin, and Turner (2022) in Mexico City. The authors of that study wanted to link remotely sensed imagery to urban policy in order to identify areas that may be vulnerable to future informal expansion. However, their method was not possible without high-resolution time-series data. It would be interesting for the City of Cape Town to explore developing a similar model using Worldview imagery, which is high-resolution imagery that has to be purchased. A fairly straightforward approach would be to train a classification model on sites that have recently been invaded, along with the areas adjacent to them, and use the model to identify similar areas. These sites could then be evaluated qualitatively to determine whether they should be given additional protection.\nIssues around informality are complex, and there is a lot of nuance that I’ve left out above. The city’s first obligation has to be addressing the root causes that are driving increasing levels of informality. But it’s also true that, as more sites are invaded, the city’s ability to do that is compromised, as it has less land to work with to deliver the infrastructure that’s needed to address the bigger problem. I think that a classification algorithm trained on high-resolution imagery could be a simple intervention that helps to identify which areas are in need of additional protection."
  },
  {
    "objectID": "week 4.html#reflections",
    "href": "week 4.html#reflections",
    "title": "4  Week 4",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nI mentioned earlier that the MSDF is a requirement of national legislation. In South Africa, municipalities are required by law to have something called an Integrated Development Plan (IDP), which sets out the budgeting priorities for the municipality. The IDP is renewed every five years. An MSDF – which is a separate document – is part of the IDP, and gives spatial expression to the policies described there. It’s meant as a long-term plan, and operates according to a 10-or-20-year vision. This is where things get a bit weird, because the MSDF is part of the IDP, but the IDP operates on a shorter timeline than the MSDF.\nI used to work in provincial government, and we’d offer support to the 24 municipalities in the Western Cape. In March/April every year there was always a big headache trying to make sure that all municipalities re-approved their IDPs and MSDFs, in order to ensure that they were legislatively compliant and protected from litigation in the event of disputes over land-use decisions (Western Cape municipalities are home to some particularly litigious residents).\nAnyway, this is a long-winded way of saying that a big part of the job of a spatial planner is ensuring that the policies and interventions they propose are compliant with legislation or other policy, and that this can mean that much of the work of policymakers is actually spent on this rather than the content. That’s also why remote sensing data has a lot of potential: it can really enhance and speed up the process of obtaining information about how a city is developing, which is critical to developing sound policy.\n\n\n\n\nCity of Cape Town. 2022. “City of Cape Town Municipal Spatial Development Framework.”\n\n\nRepublic of South Africa. 2013. “Spatial Planning and Land Use Management Act 16 of 2013,” 37.\n\n\nShoko, Moreblessings, and Julian Lloyd Smit. 2022. “LiDAR Derived Shack Footprint for the City of Cape Town, South Africa.” South African Geographical Journal 104 (1): 1–15. https://doi.org/10.1080/03736245.2020.1863253.\n\n\nTellman, Beth, Hallie Eakin, and B. L. Turner. 2022. “Identifying, Projecting, and Evaluating Informal Urban Expansion Spatial Patterns.” Journal of Land Use Science 17 (1): 100–112. https://doi.org/10.1080/1747423X.2021.2020919."
  },
  {
    "objectID": "week 5.html",
    "href": "week 5.html",
    "title": "5  Week 5",
    "section": "",
    "text": "6 Week 5: Google Earth Engine"
  },
  {
    "objectID": "week 5.html#summary",
    "href": "week 5.html#summary",
    "title": "5  Week 5",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week we dived into Google Earth Engine, which has dramatically accelerated the field of remote sensing. I have to say that it was a relief to start using Earth Engine – I found the process of having to manually download and process images in SNAP or QGIS quite cumbersome and time-consuming. That’s the main advantage of Earth Engine: the fact that it dramatically reduces processing time and eases computational resources.\nThis week we covered “reducing” – essentially, aggregation functions to provide a particular value (such as the median) for each pixel, as well as linear regression and spatial joins. We also ran PCA, which was eye-opening after week three when I ran PCA in R. This time, it ran in a few seconds.\nAnyway, here’s the NDVI for Cape Town using landsat imagery from January 2022. I think that this has returned a better result than the NDVI measure I calculated in Week 3: Here, the highest values are on table mountain and in the mountainous area in the Southeast – these are nature reserves covered in indigenous fynbos vegetation. There’s also a scar visible in Cape Point, the Southwestern peninsula, from a fire that occurred at the peak of the drought in 2018:\n\nAnd here is the GLCM texture measure from the same imagery = this looks to be highlighting high reflectance areas like industrial activity, informal areas, and that same scar from the fire:\n\nAnd finally the first principal component from PCA:\n\nThe biggest takeaway from this was how quickly all of this ran – when I performed PCA in Week 3, I started the process and went out for a run because it was taking so long to finish. In Earth Engine, I ran all of the analyses above (NDVI, texture, and PCA) in a few minutes."
  },
  {
    "objectID": "week 5.html#applications",
    "href": "week 5.html#applications",
    "title": "5  Week 5",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nThe applications for Google Earth Engine are almost limitless! Earth Engine really changed things by making remote sensing analysis faster and more accessible (you don’t have to download huge amounts of data and then spend ages processing giant raster files). So the applications are not necessarily different to what was possible before, but it has really opened up the scale and creativity of analysis. This image below sums up the range of applications that are possible with Earth Engine:\n\n\n\nGoogle Earth Engine applications (IEEE Xplore)\n\n\nEarth Engine has made planetary-scale analysis and monitoring of all sorts of phenomena possible. One example is Global Forest Watch, where you can easily view changes in tree cover for the entire planet at a range of temporal scales.\nOne area that I think is really interesting is the way Earth Engine has opened up the use of near real-time remotely sensed data. An example is Map of Life, which predicts the locations for habitats of endangered species using Earth Engine. The map is updated on-the-fly as new data comes in.\nSadly, an aspect of Earth Engine that is limited is that it’s not possible to use SAR phase data for InSAR analysis in Earth Engine. That’s quite frustrating, because I think it would be really interesting to use phase data (which uses the change in a radio wave’s phase instead of amplitude) to monitor changes in height in informal settlements. There’s been a lot of informal vertical densification in Cape Town over the last few years, and using SAR phase data to monitor the change in height over time would be an interesting way to understand and quantify the extent of this. That’s not to say it wouldn’t be possible to do this, but one would have to use SNAP or similar software in order to do it with phase data at the moment."
  },
  {
    "objectID": "week 5.html#reflections",
    "href": "week 5.html#reflections",
    "title": "5  Week 5",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nEarth engine is really fun! I have to be honest and admit that I’ve been waiting most of the module to get into it. I like how easy to use it is, and the fact that you can get straight into working with analysis-ready data. I was worried about using Javascript at first, but the amount of Javascript you need is not too extensive, and there is a wealth of tutorials out there. There are some things that I find peculiar – like the fact that you can’t run code chunks and have to run the whole script every time – but I think that’s just part of learning and adapting.\nIt’s also strange how simple things like adding a legend turn out to be quite difficult in Earth Engine. It’s possible, but it involves a lot of extra work and an additional script for each legend, so I’ve left them out for my outputs in this learning diary (my apologies in advance for this).\nI guess the biggest systemic risk with Earth Engine is that Google could decide to pull the product at any time. At the moment it seems too good to be true – it’s such a powerful service, and it’s completely free for individual users. That doesn’t seem like a sustainable business model for Google, and I suspect that it’s a matter of time before they start charging for access. Until then, I’ll continue to enjoy using it."
  },
  {
    "objectID": "week 6.html",
    "href": "week 6.html",
    "title": "6  Week 6",
    "section": "",
    "text": "7 Week 6: Classification 1"
  },
  {
    "objectID": "week 6.html#summary",
    "href": "week 6.html#summary",
    "title": "6  Week 6",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week we got into machine learning with Google Earth Engine. We covered a range of supervised machine learning tools – from simple linear regression (which is a very basic form of machine learning) to Classification and Regression Trees (CART) and Random Forest methods. Earth Engine lets you run all of these methods in just a few lines of code, and they provide a very good way to predict or classify what is happening in an area using remotely sensed data.\nThis feels like the area where Earth Engine really comes into its own: it’s amazing to have such a range of analysis-ready imagery readily available and to be able to run these algorithms so quickly. I’ll discuss CART and Random Forest here, before getting into some of the ways they can be applied.\n\n7.1.1 CART\nCART works by subsetting the data in a series of forks. Each fork is split into a predictor variable and each node has a prediction at the end, as shown below:\n\n\n\nCART (Geeks for Geeks)\n\n\nCART can be used on both categorical (classification), as well as continuous (regression) outcome variables. The algorithm splits the data in order to have the lowest mean square error (for continuous data) or gini impurity (for categorical data) in each split. The way that I understand this intuitively is that the model will select the split at each point that minimises the variance in the resulting subsets. In the case of categorical variables, the value for each subset is the majority class of the response variable falling into that category; in the case of continuous data, the output is the mean of the records in the subset. The algorithm will run through this process until it reaches a pre-defined stopping criteria.\nSo unlike linear regression, where you’re trying to arrive at a single coefficient which estimates the slope of a relationship, CART outputs a series of mean values for different subsets of the data (in the case of a continuous outcome variable). This makes it useful for working with nonlinear datasets; the art of using CART is in deciding when and where to stop splitting the data. It’s possible to keep going all the way down to individual records in the dataset, in which case the model will be perfectly predictive, but then it is unlikely to generalise well to unseen data as the model will be overfit on the dataset it has been trained on. Conversely, if there are too many records in each subset, the model is likely to be unreliable as a predictor as it will be underfit. This image shows how a decision tree algorithm might subset a two-dimensional dataset with a continuous outcome variable:\n\n\n\nDecision tree subsetting (source: https://www.datacamp.com/tutorial/decision-trees-R)\n\n\nHere’s an example of a CART classifier on land use in Cape Town, using Sentinel 2 data (I’ve zoomed in so that a bit more of the detail is visible):\n\n\n\nCART classification in Cape Town on Sentinel 2 Imagery\n\n\n\n\n7.1.2 Random Forest\nWhere decision trees really come into their own is when many trees are used together as part of an ensemble. This is called a random forest. A random forest works by creating a whole lot of subsets of the training data, and creating a decision tree for each subset. The results for each subset are weighed together to arrive at a final prediction. While one tree might produce unreliable results, this can be overcome by creating many trees!\nHere are the results of a Random Forest Classifier on the same imagery:\n\n\n\nA random forest classifier for Cape Town imagery (Green: Mountain, pink: urban, purple: water, grey: industrial)\n\n\nThere isn’t a huge difference between either image – there’s definitely more noise in the CART output, and you can see more of the structure of the city coming through from the Random Forest model. These aren’t perfect land use classifications by any means, but it’s a pretty impressive result considering that the input classifiers were just some polygons I selected on the true colour composite. It’s easy to see how this could quickly be turned into a useful land use/land cover output with better input classifications.\n\n7.1.2.1 Accuracy\nThere are a couple of ways to measure the accuracy of this model. The one I have generated returns an Out of Bag Error Estimate of 14.5%. This is fairly close to the model’s validation accuracy (its performance on unseen data), which was 0.87%. The training accuracy was much higher, at 99.6% (this is unsurprising as it’s the result of the model on the data it was trained on). I’ll talk more about accuracy measures next week."
  },
  {
    "objectID": "week 6.html#applications",
    "href": "week 6.html#applications",
    "title": "6  Week 6",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nThere are many ways that these models can and have been applied. There are applications around land cover and land use that I’ve described above.\nMann, Melaas, and Malik (2016) used a Random Forest model to predict instances of power outages in India from nighttime Visible Infrared Imaging Radiometer Suite (VIIRS) imagery, which shows lights on earth at night. They did this by training the model on known instances of power outages using voltage monitoring data from an NGO. A map of their output is shown below. I think this kind of approach would be extremely powerful in South Africa, where we’ve had persistent rolling blackouts since 2015. This image shows one of the outputs of that model:\n\n\n\nPredicted electricity outages in India using a random forest model and VIIRS imagery. Source: Mann, Melaas, and Malik (2016)\n\n\nElsewhere, Cheng, Tian, and Yin (2022) used a Random Forest model and Earth Engine to identify factors that explain urban land expansion in China. It found that elevation, GDP, and per capita income were some of the factors that explained urban expansion in the area in question. I think that this suggests an extension of the approach to the problem of identifying sites vulnerable to informal invasion in Cape Town that I discussed in Week 4: including socioeconomic, topographical, and demographic variables in the model."
  },
  {
    "objectID": "week 6.html#reflections",
    "href": "week 6.html#reflections",
    "title": "6  Week 6",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThese techniques are clearly extremely powerful, and the applications I’ve described above can be employed to answer very relevant policy questions in South Africa. The big concern with all of these models is the data that they are supervised on – it’s very easy to train a model on data that is biased in some way, with the output being accordingly influenced. Given that I’ve described uses that apply to informal contexts, this is a particular concern, especially given the history of racial bias in machine learning algorithms.\nSimilarly, interpreting the output of a decision tree classifier or random forest is not nearly as simple as reading the coefficient and intercept from a linear regression model. This also has challenges in the policy space, as it’s not possible to succinctly describe which inputs are influencing the outcome variable based on the results of one of these algorithms. Nevertheless, they are very powerful, and they certainly have their place in shaping better policy.\nOn another note, it’s fun to start to put these techniques into practice. I’ve covered decision trees and random forests in other modules, but in a mostly theoretical context. It’s interesting to see how they can actually be applied to remotely sensed data, and I’m really impressed by how well the random forest classifier started to pick up the structure of Cape Town above – I didn’t visualise it very well, and it definitely needs better training inputs (see my note about bias above), but it’s definitely starting to create a coherent picture of Cape Town’s defining urban features.\n\n\n\n\nCheng, Lin-Lin, Chao Tian, and Ting-Ting Yin. 2022. “Identifying Driving Factors of Urban Land Expansion Using Google Earth Engine and Machine-Learning Approaches in Mentougou District, China.” Scientific Reports 12 (1): 16248. https://doi.org/10.1038/s41598-022-20478-z.\n\n\nMann, Michael, Eli Melaas, and Arun Malik. 2016. “Using VIIRS Day/Night Band to Measure Electricity Supply Reliability: Preliminary Results from Maharashtra, India.” Remote Sensing 8 (9): 711. https://doi.org/10.3390/rs8090711."
  },
  {
    "objectID": "week 7.html",
    "href": "week 7.html",
    "title": "7  Week 7",
    "section": "",
    "text": "8 Week 7: Classification - The Big Questions"
  },
  {
    "objectID": "week 7.html#summary",
    "href": "week 7.html#summary",
    "title": "7  Week 7",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nThis week we continued with more classification algorithms in GEE. We focused on object-based image analysis and sub-pixel analysis, which I will describe below.\n\n8.1.1 Sub-pixel Analysis\nThis determines the proportion of each landcover type per pixel. Essentially, it works by comparing the reflectance values in each pixel to the ‘ideal’ values of a spectrally pure endmember for that landcover class. The algorithm then infers what is likely to be the dominant landcover class in that pixel, based on probability. Usually, spectrally pure endmembers will be determined through laboratory measurements or controlled experiments. Below is an example of running sub-pixel analysis on Cape Town using polygons that I drew on a Landsat colour composite to classify landcover. From visual inspection, the algorithm performed reasonably well, although large stretches of the nature reserve in Cape Point (the peninsula in the South) have been classified as urban.\n\n\n\nSub pixel analysis. Green: Natural vegetation; Pink: Urban; Grey: Dry grass\n\n\n\n\n8.1.2 Object-based Image Analysis\nObject-based image analysis is almost the inverse of sub-pixel analysis: Instead of trying to determine what the landcover class within each pixel might be, it groups pixels together. It does this by identifying areas that are similar to each other and creating shapes of similar (homogeneous) or different (heterogeneous) ‘superpixels’. Here is the output of an object-based image analysis that used Simple Non-Iterative Clustering (SNIC) to create a grid of super pixels (groups of pixels). It then assigned landcover classes using CART:\n\n\n\nSuper pixel analysis. Green: Natural vegetation; Pink: Urban; Grey: Dry grass\n\n\nFrom visual inspection, this actually performed better in identifying that Cape Point is not urban, but it has probably under-estimated the amount of natural vegetation and urban area in the metro as a whole.\nIn order to formally assess the accuracy I would need to split the data into a training and testing set last and evaluate its performance. I haven’t done that, but I outline some of the means for evaluating performance below.\n\n\n8.1.3 Accuracy\nThere are a number of ways to measure the accuracy of a model in remote sensing and machine learning. Which measure (or combination of measures) you choose depends on the application, but some of the most common are user’s accuracy, producer’s accuracy and F1 score. These are all derived from a confusion matrix – basically, a table that shows the number of correctly and incorrectly classified pixels for each class.\nThe user’s accuracy is the proportion of pixels that are correctly classified as a class out of all pixels that are classified as that class. It indicates how reliable the classification map is for a given class. It’s also called precision, and it’s calculated as follows:\n\\[User's\\ Accuracy= \\frac{True\\ Positive}{True\\ Positive+False\\ Positive}\\]\nThe producer’s accuracy is the proportion of pixels that are correctly classified as a class out of all pixels that belong to that class in reality. It indicates how well the classification map represents a given class. It’s also called recall, and it’s calculated like this:\n\\[Producer's\\ Accuracy= \\frac{True\\ Positive}{True\\ Positive+False\\ Negative}\\]\nThe f1 score is the harmonic mean of user’s accuracy and producer’s accuracy for a class. It balances both measures and gives a single value that balances both. It’s calculated like this:\n\\[F1= \\frac{2 * User's\\ Accuracy * Producer's\\ Accuracy}{User's\\ Accuracy + Producer's\\ Accuracy}\\]\nI prefer the terms producer’s accuracy and user’s accuracy because they make more intuitive sense to me, but precision and recall are more commonly used outside of a remote sensing context."
  },
  {
    "objectID": "week 7.html#applications",
    "href": "week 7.html#applications",
    "title": "7  Week 7",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nAdvanced classification methods for land use and land cover have really expanded recently. Brown et al. (2022) created a model that updates global land use and land cover in near real-time. They use a deep learning algorithm to do it, and it’s hosted on Earth Engine – you can take a look here.\nElsewhere, sub-pixel analysis has been used for urban inundation mapping (essentially, maps of flooded areas in cities). Li et al. (2020) use deep learning in combination wiht Elman neural networks for sub-pixel analysis to improve the accuracy of flood mapping using Landsat imagery.\nElsewhere, Object-Based Image Analysis has recently been used to map informal settlements by Matarira et al. (2022). They used both optical imagery from Sentinel-2 and Planetscope, along with SAR data from Sentinel-1, to identify informal settlements in Durban, South Africa."
  },
  {
    "objectID": "week 7.html#reflections",
    "href": "week 7.html#reflections",
    "title": "7  Week 7",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nIt’s quite incredible to see how quickly the possibilities of remote sensing are expanding. Google Earth Engine has really opened up the possibilities, and it feels like there’s been a real explosion in the discipline in recent years. I think this is both exciting and intimidating: these recent advances are so significant, and have such obvious utility, that it feels really exciting to be able to understand and undertake some of them. The paper that I described above that identifies informal settlements in Durban, for example, employs methods that I have learnt on this course and it doesn’t feel like a massive stretch for me to be able to do something similar.\nAt the same time, I feel like I’m just scratching the surface – and the world of possibilities is really dizzying. I theoretically understand what Li et al. (2020) have done for inundation mapping, although it’s a stretch, but the deep learning methods (and data infrastructure and pipelines) employed by Brown et al. (2022) are just dizzying. I guess it’s cool that I can really appreciate it now, at least, and I’m really pleased that I’ve started to gain some real applied insight into how to use remote sensing data – because I really think it has incredibly powerful applications for cities.\n\n\n\n\nBrown, Christopher F., Steven P. Brumby, Brookie Guzder-Williams, Tanya Birch, Samantha Brooks Hyde, Joseph Mazzariello, Wanda Czerwinski, et al. 2022. “Dynamic World, Near Real-Time Global 10 m Land Use Land Cover Mapping.” Scientific Data 9 (1): 251. https://doi.org/10.1038/s41597-022-01307-4.\n\n\nLi, Linyi, Yun Chen, Tingbao Xu, Lingkui Meng, Chang Huang, and Kaifang Shi. 2020. “Spatial Attraction Models Coupled with Elman Neural Networks for Enhancing Sub-Pixel Urban Inundation Mapping.” Remote Sensing 12 (13): 2068. https://doi.org/10.3390/rs12132068.\n\n\nMatarira, Dadirai, Onisimo Mutanga, Maheshvari Naidu, and Marco Vizzari. 2022. “Object-Based Informal Settlement Mapping in Google Earth Engine Using the Integration of Sentinel-1, Sentinel-2, and PlanetScope Satellite Data.” Land 12 (1): 99. https://doi.org/10.3390/land12010099."
  },
  {
    "objectID": "week 8.html",
    "href": "week 8.html",
    "title": "8  Week 8",
    "section": "",
    "text": "9 Temperature"
  },
  {
    "objectID": "week 8.html#summary",
    "href": "week 8.html#summary",
    "title": "8  Week 8",
    "section": "9.1 Summary",
    "text": "9.1 Summary\nThis week we covered the role that remote sensing can play in helping cities adapt to and mitigate rising temperatures. The Urban Heat Island is a well-documented phenomenon. It refers to the tendency for built-up areas to have higher temperatures than their surrounds. This is a pretty significant problem, given that the majority of the world’s population live in cities.\nA key part of the issue is that urban heat islands tend to produce positive feedback loops. This means that they are self-reinforcing, and the effect gets worse over time. Different aspects of the built environment interact with each other and magnify the heating effects of other factors. This image illustrates part of the urban heat island effect: Built-up areas have lower rates of evapotranspiration, and tall buildings trap ambient heat. These two mechanisms combine for a larger heating effect:\n\n\n\nSource: Public Health Notes\n\n\nFrom a policy perspective, remote sensing data provides a number of advantages in detecting, understanding, monitoring, and intervening to reduce this effect.\nThere are a number of satellites that capture temperature data, including Landsat and the Moderate Resolution Imaging Spectroradiometer (MODIS). MODIS has a high temporal resolution, which allows for 1-2 images a day in most parts of the world.\nHere I’ve calculated mean temperature for the Western Cape using both Landsat and MODIS:\nLandsat:\n\n\n\nMean Landsat temperature for the Western Cape in 2022 (lighter colours represent higher temperatures).\n\n\nMODIS:\n\n\n\nMean MODIS temperature for the Western Cape in 2022 (lighter colours represent higher temperatures).\n\n\nYou can see that MODIS captures a lot more of the variation in temperature than Landsat in this case. Interestingly, the MODIS temperature data picks up changes in elevation pretty well: You can see the Cape Fold mountains very clearly in the image above.\nAnother interesting dimension, given all of the discussion above about urban heat island effects, is that Cape Town doesn’t really register as having a high temperature in either image. That isn’t all that surprising, however: the bright area in the North is Namaqualand, which is a desert, and the one in the North East is the Karoo, an arid semi-desert region that has been in the grips of a prolonged multi-year drought."
  },
  {
    "objectID": "week 8.html#applications",
    "href": "week 8.html#applications",
    "title": "8  Week 8",
    "section": "9.2 Applications",
    "text": "9.2 Applications\nIn South Africa, the Council for Scientific and Industrial Research (CSIR) have developed a model for understanding temperature in cities. Their model uses temperature projections and spatial information from municipal spatial development frameworks (see week 4) about urban surfaces to understand how rising temperatures might be mitigated in different cities. The CSIR website doesn’t say whether remote sensing data was used in their model, but there is a clear case for integrating it if it doesn’t, and it would be interesting to see how the model outputs might change if its predictions were based on remote sensing inputs rather than global temperature projections.\nHowever, it’s also important to have a full understanding of the phenomenon under study, as Parlow (2021) discusses. In this study, the authors note that many of the current papers simplify the urban heat island effect by failing to account for how the data was collected or what it represents. There are several pitfalls to this approach, one of which is the fact that most remotely sensed data will be reading temperature at the roof level. This image shows this effect:\n\n\n\nLeft: Surface temperatures in Basel. Middle: Sky-view factor (visible sky above a pixel). Right: Percentage of roof cover (Parlow 2021)\n\n\nIt’s easy to see how this could lead to inaccurate results, as high temperatures on rooftops don’t necessarily mean that people on the ground are experiencing elevated temperatures! That’s not to say that the urban heat island effect doesn’t exist – this is not what the paper is trying to say – but it is to stress the importance of ground-truthing remotely sensed data, and having a full understanding of how data was collected and what it really shows."
  },
  {
    "objectID": "week 8.html#reflections",
    "href": "week 8.html#reflections",
    "title": "8  Week 8",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections\nI find this complicated - as with so many urban issues, it’s difficult to drive systemic change when cities are created by so many small decisions. Cities are built one land use decision at a time, and that’s especially the case with something like the urban heat island effect: the cumulative impact of many individual decisions has led to this scenario.\nI remember when, a few years ago, the block of flats that I lived in was due to be repainted. The residents wanted to go with a darker colour because it was fashionable. I tried to object on the basis of the heat impact of the darker colour (I also just didn’t think it looked very nice, so my argument was biased), but there was very little interest in that argument from other residents. I think that’s the case with a lot of this: it’s difficult to understand, in the abstract, how a decision like choosing a paint colour for your building will contribute to global warming. However, when many people think that way, and these small decisions reinforce each other, it adds up to a positive feedback loop that is very difficult to arrest.\nI think that’s a really big advantage to remote sensing data being applied in this way: It makes it possible to quantify existing trends, and model how different interventions will change things in the future.\nAs a final fun reflection, here’s a link to a video my classmates and I made to explain the urban heat island effect with interpretive dance!\n\n\n\n\nParlow, Eberhard. 2021. “Regarding Some Pitfalls in Urban Heat Island Studies Using Remote Sensing Technology.” Remote Sensing 13 (18): 3598. https://doi.org/10.3390/rs13183598."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barba-Sevilla, Magali, and Elizabeth Ashley Menezes. 2023. “GUIDE:\nFUNDAMENTALS OF SYNTHETIC APERTURE RADAR (SAR).” https://storymaps.arcgis.com/stories/20d8cd2ce11a4d5d81a8a65711d5ec29.\n\n\nBrown, Christopher F., Steven P. Brumby, Brookie Guzder-Williams, Tanya\nBirch, Samantha Brooks Hyde, Joseph Mazzariello, Wanda Czerwinski, et\nal. 2022. “Dynamic World, Near Real-Time\nGlobal 10 m Land Use Land Cover Mapping.” Scientific\nData 9 (1): 251. https://doi.org/10.1038/s41597-022-01307-4.\n\n\nCheng, Lin-Lin, Chao Tian, and Ting-Ting Yin. 2022. “Identifying\nDriving Factors of Urban Land Expansion Using Google Earth\nEngine and Machine-Learning Approaches in Mentougou\nDistrict, China.” Scientific Reports\n12 (1): 16248. https://doi.org/10.1038/s41598-022-20478-z.\n\n\nCity of Cape Town. 2022. “City of Cape Town Municipal\nSpatial Development Framework.”\n\n\nGeospatial UK. 2021. “What Is Remote Sensing?” https://www.geospatialuk.org/post/what-is-remote-sensing.\n\n\nLi, Linyi, Yun Chen, Tingbao Xu, Lingkui Meng, Chang Huang, and Kaifang\nShi. 2020. “Spatial Attraction Models Coupled with\nElman Neural Networks for Enhancing Sub-Pixel Urban\nInundation Mapping.” Remote Sensing 12 (13):\n2068. https://doi.org/10.3390/rs12132068.\n\n\nMann, Michael, Eli Melaas, and Arun Malik. 2016. “Using\nVIIRS Day/Night Band to Measure\nElectricity Supply Reliability: Preliminary Results\nfrom Maharashtra, India.” Remote\nSensing 8 (9): 711. https://doi.org/10.3390/rs8090711.\n\n\nMatarira, Dadirai, Onisimo Mutanga, Maheshvari Naidu, and Marco Vizzari.\n2022. “Object-Based Informal Settlement Mapping in\nGoogle Earth Engine Using the Integration of\nSentinel-1, Sentinel-2, and PlanetScope\nSatellite Data.” Land 12 (1): 99. https://doi.org/10.3390/land12010099.\n\n\nMunyati, Christopher. 2004. “Use of Principal Component\nAnalysis (PCA) of Remote Sensing Images\nin Wetland Change Detection on the Kafue\nFlats, Zambia.” Geocarto\nInternational 19 (3): 11–22. https://doi.org/10.1080/10106040408542313.\n\n\nParlow, Eberhard. 2021. “Regarding Some Pitfalls in\nUrban Heat Island Studies Using Remote Sensing\nTechnology.” Remote Sensing 13 (18): 3598. https://doi.org/10.3390/rs13183598.\n\n\nRepublic of South Africa. 2013. “Spatial Planning and\nLand Use Management Act 16 of 2013,” 37.\n\n\nShoko, Moreblessings, and Julian Lloyd Smit. 2022.\n“LiDAR Derived Shack Footprint for the\nCity of Cape Town, South\nAfrica.” South African Geographical Journal 104\n(1): 1–15. https://doi.org/10.1080/03736245.2020.1863253.\n\n\nTellman, Beth, Hallie Eakin, and B. L. Turner. 2022. “Identifying,\nProjecting, and Evaluating Informal Urban Expansion Spatial\nPatterns.” Journal of Land Use Science 17 (1): 100–112.\nhttps://doi.org/10.1080/1747423X.2021.2020919."
  }
]